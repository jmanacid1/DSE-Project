{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chardet as chardet\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: ascii\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('Data/BHCF20250331.txt', 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        encoding = result['encoding']\n",
    "        print(f\"Detected encoding: {encoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: ascii\n"
     ]
    }
   ],
   "source": [
    "raw_data = open('Data/BHCF20250331.txt', 'rb').read()\n",
    "result = chardet.detect(raw_data)\n",
    "encoding = result['encoding']\n",
    "print(f\"Detected encoding: {encoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     df_list\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(encoding,df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 19\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import glob as glob\n",
    "all_files = glob.glob('Data/BHCF*.txt')\n",
    "all_files_2008 = glob.glob('Data_Processing/inputs/FED_data/BHCF200[7-9]*.txt')\n",
    "print(all_files_2008)\n",
    "\n",
    "df_list = []\n",
    "for file_path in all_files_2008:\n",
    "    print(file_path)\n",
    "    raw_data = open(file_path, 'rb').read()\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "    df = pd.read_csv(file_path, delimiter = '^', encoding=encoding, engine='python', on_bad_lines='skip')\n",
    "\n",
    "    #Potential Row Level Filtering on Tier 1 Capital Ratio\n",
    "    df = df[df['BHCA7206'].isna() == False]\n",
    "\n",
    "    df_list.append(df)\n",
    "    print(encoding,df.shape)\n",
    "combined_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_Processing/inputs/FED_data/BHCF20070331/BHCF20070331.txt\n",
      "ascii (1218, 1476)\n",
      "Data_Processing/inputs/FED_data/BHCF20070630/BHCF20070630.txt\n",
      "ascii (5688, 1477)\n",
      "Data_Processing/inputs/FED_data/BHCF20070930/BHCF20070930.txt\n",
      "ascii (1208, 1477)\n",
      "Data_Processing/inputs/FED_data/BHCF20071231/BHCF20071231.txt\n",
      "ascii (5670, 1477)\n",
      "Data_Processing/inputs/FED_data/BHCF20080331/BHCF20080331.txt\n",
      "ascii (1218, 1639)\n",
      "Data_Processing/inputs/FED_data/BHCF20080630/BHCF20080630.txt\n",
      "ascii (5602, 1642)\n",
      "Data_Processing/inputs/FED_data/BHCF20080930/BHCF20080930.txt\n",
      "ascii (1217, 1642)\n",
      "Data_Processing/inputs/FED_data/BHCF20081231/BHCF20081231.txt\n",
      "ascii (5537, 1642)\n",
      "Data_Processing/inputs/FED_data/BHCF20090331/BHCF20090331.txt\n",
      "ascii (1281, 1670)\n",
      "Data_Processing/inputs/FED_data/BHCF20090630/BHCF20090630.txt\n",
      "ascii (5510, 1924)\n",
      "Data_Processing/inputs/FED_data/BHCF20090930/BHCF20090930.txt\n",
      "ascii (1265, 1926)\n",
      "Data_Processing/inputs/FED_data/BHCF20091231/BHCF20091231.txt\n",
      "ascii (5434, 1927)\n",
      "Data_Processing/inputs/FED_data/BHCF20100331/BHCF20100331.txt\n",
      "ascii (1294, 1942)\n",
      "Data_Processing/inputs/FED_data/BHCF20100630/BHCF20100630.txt\n",
      "ascii (5346, 1942)\n",
      "Data_Processing/inputs/FED_data/BHCF20100930/BHCF20100930.txt\n",
      "ascii (1268, 1940)\n",
      "Data_Processing/inputs/FED_data/BHCF20101231/BHCF20101231.txt\n",
      "ascii (5268, 1944)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base = Path(\"Data_Processing/inputs/FED_data\")\n",
    "\n",
    "files = []\n",
    "for year in range(2007, 2011):  # 2007â€“2010 inclusive\n",
    "    pattern = f\"BHCF{year}*/*.txt\"\n",
    "    files.extend(base.glob(pattern))\n",
    "\n",
    "# If you want a sorted list:\n",
    "files = sorted(files)\n",
    "\n",
    "# Print or inspect\n",
    "#for f in files:\n",
    "    #print(f)\n",
    "\n",
    "df_list = []\n",
    "for file_path in files:\n",
    "    print(file_path)\n",
    "    raw_data = open(file_path, 'rb').read()\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "    df = pd.read_csv(file_path, delimiter = '^', encoding=encoding, engine='python', on_bad_lines='skip')\n",
    "\n",
    "    #Potential Row Level Filtering on Tier 1 Capital Ratio\n",
    "    #df = df[df['BHCA7206'].isna() == False]\n",
    "\n",
    "    df_list.append(df)\n",
    "    print(encoding,df.shape)\n",
    "combined_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54024, 2047)\n"
     ]
    }
   ],
   "source": [
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dddf = pd.read_csv('/Users/jacksonlipfert/Downloads/MDRM/MDRM_CSV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/1lp6v8rx7hg46v35nrbf4mk00000gn/T/ipykernel_3096/756459364.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  short_dddf['Full Code'] = short_dddf['Mnemonic'] + short_dddf['Item Code']\n"
     ]
    }
   ],
   "source": [
    "#dddf.head()\n",
    "short_dddf = dddf[['Mnemonic','Item Code','Item Name']]\n",
    "\n",
    "\n",
    "short_dddf['Full Code'] = short_dddf['Mnemonic'] + short_dddf['Item Code']\n",
    "important_codes = ['RSSD', 'BHBC', 'BHCA', 'BHCB', 'BHCK', 'BHCM', 'BHCP', 'BHCT', 'BHCW',\n",
    "       'BHCX', 'BHCY', 'BHDM', 'BHFN', 'BHOD', 'BHPA', 'BHPX', 'BHSP', 'BHSX',\n",
    "       'BHTX', 'TEXT']\n",
    "short_dddf = short_dddf[short_dddf['Mnemonic'].isin(important_codes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIER 1 RISK-BASED CAPITAL RATIO: BHCA7206\n",
      "TIER 1 RISK-BASED CAPITAL RATIO: BHCK7206\n",
      "TIER 1 RISK-BASED CAPITAL RATIO: BHCW7206\n"
     ]
    }
   ],
   "source": [
    "# create mapper\n",
    "\n",
    "\n",
    "column_mapper = {}\n",
    "\n",
    "#Columns that appear across multiple types of companies with the same balance sheet name, need to keep code to disambiguate\n",
    "special_columns = [\n",
    "    'TOTAL ASSETS',\n",
    "    'TOTAL EQUITY',\n",
    "    'TOTAL EQUITY CAPITAL',\n",
    "    'NET INCOME',\n",
    "    'NET INCOME (LOSS)'\n",
    "    ]\n",
    "\n",
    "def get_name_from_full_code(fullcode, df):\n",
    "    '''get column name from full code found in data dictionary'''\n",
    "    column = df['Item Name'][df['Full Code'] == fullcode]\n",
    "    col_name = column.iloc[0]\n",
    "    return col_name\n",
    "\n",
    "\n",
    "#Loop over short data dictionary and populate column mapper with name\n",
    "for code in short_dddf['Full Code']:\n",
    "    column_mapper[code] = get_name_from_full_code(code,short_dddf)\n",
    "\n",
    "\n",
    "def disambiguate_column_mapper(column_mapper, special_columns):\n",
    "    '''Loop over column mapper and replace ambigious columns (found in special_columns)'''\n",
    "\n",
    "    for key,value in column_mapper.items():\n",
    "        if value in special_columns:\n",
    "            column_mapper[key] = f'{value}: {key}' \n",
    "    return column_mapper\n",
    "\n",
    "def disambiguate_all_column_mapper(column_mapper):\n",
    "    '''Loop over column mapper and replace ambigious columns (found in special_columns)'''\n",
    "\n",
    "    for key,value in column_mapper.items():\n",
    "        column_mapper[key] = f'{value}: {key}' \n",
    "    return column_mapper\n",
    "\n",
    "#Final column mapper\n",
    "#disambig_column_mapper = disambiguate_column_mapper(column_mapper, special_columns)\n",
    "disambig_column_mapper = disambiguate_all_column_mapper(column_mapper)\n",
    "\n",
    "#check to make sure columns have been renamed\n",
    "column_name_in_question = 'TIER 1 RISK-BASED CAPITAL RATIO'\n",
    "for key,value in disambig_column_mapper.items():\n",
    "    if column_name_in_question in str(value):\n",
    "        print(value)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54024, 2047)\n"
     ]
    }
   ],
   "source": [
    "renamed_combined_df = combined_df.rename(columns=disambig_column_mapper)\n",
    "print(renamed_combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LIABILITY FOR SHORT POSITIONS: EQUITY SECURITIES: BHCKG209',\n",
      "       'LIABILITY FOR SHORT POSITIONS: DEBT SECURITIES: BHCKG210',\n",
      "       'LIABILITY FOR SHORT POSITIONS: ALL OTHER ASSETS: BHCKG211',\n",
      "       'CASH FLOWS FROM FINANCING ACTIVITIES: NET CHANGE IN PURCHASED FUNDS AND OTHER SHORT-TERM BORROWINGS: BHCPF818',\n",
      "       'SHORT-TERM BORROWINGS INCLUDED IN ITEM 3621 FROM PARENT BANK HOLDING COMPANY(S): BHSP3524',\n",
      "       'SHORT-TERM BORROWINGS INCLUDED IN ITEM 3621 FROM SUBSIDIARY BANK HOLDING COMPANY(S): BHSP3526',\n",
      "       'ENTITY SHORT NAME: RSSD9010'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "search_string = 'SHORT'\n",
    "matching_columns = renamed_combined_df.columns[renamed_combined_df.columns.str.contains(search_string)]\n",
    "print(matching_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n"
     ]
    }
   ],
   "source": [
    "\n",
    "col = 'REPORTING DATE (CC;YR;MO;DA): RSSD9999'\n",
    "s = renamed_combined_df[col]\n",
    "\n",
    "mask = s.floordiv(10_000) == 2025          # works with nullable Int64 too\n",
    "out = renamed_combined_df['ENTITY SHORT NAME: RSSD9010'][mask]\n",
    "print(len(out.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_combined_df.to_csv('2007-2010_Bank_Data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
