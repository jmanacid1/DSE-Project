{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xf3 in position 40: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32mparsers.pyx:1120\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1272\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1285\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1535\u001b[0m, in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xf3 in position 40: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dftest \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData/BHCF20241231.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m^\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:921\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1066\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1127\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1272\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1285\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1535\u001b[0m, in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xf3 in position 40: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "dftest = pd.read_csv('Data/BHCF20241231.txt', delimiter = '^',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/1lp6v8rx7hg46v35nrbf4mk00000gn/T/ipykernel_9550/157291538.py:6: DtypeWarning: Columns (2088,2089,2091,2092,2094,2095,2098,2099,2100,2101,2132,2141,2151,2152,2153,2154,2155,2156,2157,2158,2160,2161,2162,2163,2165,2166,2167,2171,2173,2174,2177,2178,2179,2181,2183,2184,2185,2200,2201,2202,2203,2204,2205,2206,2207,2208,2215,2216,2217,2219,2220,2221,2222) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, delimiter = '^')\n",
      "/var/folders/17/1lp6v8rx7hg46v35nrbf4mk00000gn/T/ipykernel_9550/157291538.py:6: DtypeWarning: Columns (2088,2089,2092,2097,2100,2101,2141,2152,2153,2154,2157,2158,2167,2185,2201,2202,2203,2204,2205,2206,2207,2208,2216,2219,2220,2221,2222) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, delimiter = '^')\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xf3 in position 189016: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m df_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m all_files:\n\u001b[0;32m----> 6\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m^\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     df_list\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m      8\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(df_list, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:325\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xf3 in position 189016: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import glob as glob\n",
    "all_files = glob.glob('Data/BHCF*.txt')\n",
    "\n",
    "df_list = []\n",
    "for file_path in all_files:\n",
    "    df = pd.read_csv(file_path, delimiter = '^')\n",
    "    df_list.append(df)\n",
    "combined_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4266, 2224)\n"
     ]
    }
   ],
   "source": [
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/1lp6v8rx7hg46v35nrbf4mk00000gn/T/ipykernel_7843/2340049902.py:1: DtypeWarning: Columns (2088,2089,2092,2097,2100,2101,2141,2152,2153,2154,2157,2158,2167,2185,2201,2202,2203,2204,2205,2206,2207,2208,2216,2219,2220,2221,2222) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('Data/BHCF20250331.txt', delimiter = '^')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Data/BHCF20250331.txt', delimiter = '^')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/1lp6v8rx7hg46v35nrbf4mk00000gn/T/ipykernel_7843/3343375304.py:1: DtypeWarning: Columns (2088,2089,2091,2092,2094,2095,2098,2099,2100,2101,2132,2141,2151,2152,2153,2154,2155,2156,2157,2158,2160,2161,2162,2163,2165,2166,2167,2171,2173,2174,2177,2178,2179,2181,2183,2184,2185,2200,2201,2202,2203,2204,2205,2206,2207,2208,2215,2216,2217,2219,2220,2221,2222) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfq2 = pd.read_csv('Data/BHCF20250630 3.txt', delimiter = '^')\n"
     ]
    }
   ],
   "source": [
    "dfq2 = pd.read_csv('Data/BHCF20250630 3.txt', delimiter = '^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['RSSD', 'BHBC', 'BHCA', 'BHCB', 'BHCK', 'BHCM', 'BHCP', 'BHCT', 'BHCW',\n",
      "       'BHCX', 'BHCY', 'BHDM', 'BHFN', 'BHOD', 'BHPA', 'BHPX', 'BHSP', 'BHSX',\n",
      "       'BHTX', 'TEXT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.str[:4].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dddf = pd.read_csv('/Users/jacksonlipfert/Downloads/MDRM/MDRM_CSV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/1lp6v8rx7hg46v35nrbf4mk00000gn/T/ipykernel_7843/756459364.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  short_dddf['Full Code'] = short_dddf['Mnemonic'] + short_dddf['Item Code']\n"
     ]
    }
   ],
   "source": [
    "#dddf.head()\n",
    "short_dddf = dddf[['Mnemonic','Item Code','Item Name']]\n",
    "\n",
    "\n",
    "short_dddf['Full Code'] = short_dddf['Mnemonic'] + short_dddf['Item Code']\n",
    "important_codes = ['RSSD', 'BHBC', 'BHCA', 'BHCB', 'BHCK', 'BHCM', 'BHCP', 'BHCT', 'BHCW',\n",
    "       'BHCX', 'BHCY', 'BHDM', 'BHFN', 'BHOD', 'BHPA', 'BHPX', 'BHSP', 'BHSX',\n",
    "       'BHTX', 'TEXT']\n",
    "short_dddf = short_dddf[short_dddf['Mnemonic'].isin(important_codes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NET INCOME (LOSS): BHBC4340\n",
      "NET IMPACT ON NET INCOME (BEFORE TAXES) OF THE SALE OF OTHER ASSETS WITH RECOURSE\n",
      "NET IMPACT ON NET INCOME (BEFORE TAXES) OF LOAN ASSETS THAT WERE SOLD WITH RECOURSE\n",
      "NET IMPACT ON NET INCOME (BEFORE TAXES) OF DEFEASED DEBT\n",
      "NET INCOME (LOSS): BHCK4340\n",
      "NET INCOME BEFORE INCOME TAXES; EXTRAORDINARY ITEMS; AND OTHER ADJUSTMENTS ON A FULLY TAXABLE EQUIVALENT BASIS\n",
      "NET INCOME FOR ALL INSURANCE-RELATED ACTIVITIES\n",
      "NET INCOME: BHCKC246\n",
      "NET INCOME: BHCKC250\n",
      "LESS: NET INCOME (LOSS) ATTRIBUTABLE TO NONCONTROLLING (MINORITY) INTERESTS\n",
      "NET INCOME (LOSS) ATTRIBUTABLE TO BANK AND NONCONTROLLING MINORITY INTERESTS\n",
      "NET INCOME FROM SERVICE CORPORATION/SUBSIDIARIES (SEE NOTE BELOW)\n",
      "CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - PROVISION FOR DEFERRED INCOME TAXES\n",
      "CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - (GAIN) OR LOSS ON SALES OF ASSETS\n",
      "CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - EQUITY IN UNDISTRIBUTED (EARNINGS) LOSSES OF SUBSIDIARIES\n",
      "CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - EQUITY IN EXTRAORDINARY ITEMS OF SUBSIDIARIES\n",
      "CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - NET CHANGE IN OTHER LIABILITIES\n",
      "CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - NET CHANGE IN OTHER ASSETS\n",
      "CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - OTHER; NET\n",
      "CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - TOTAL ADJUSTMENTS\n",
      "NET INCOME (LOSS): BHCP4340\n",
      "NET INCOME (LOSS): BHCT4340\n",
      "NET INCOME (LOSS): BHPA4340\n",
      "NET INCOME (LOSS): BHSP4340\n"
     ]
    }
   ],
   "source": [
    "# create mapper\n",
    "\n",
    "\n",
    "column_mapper = {}\n",
    "\n",
    "#Columns that appear across multiple types of companies with the same balance sheet name, need to keep code to disambiguate\n",
    "special_columns = [\n",
    "    'TOTAL ASSETS',\n",
    "    'TOTAL EQUITY',\n",
    "    'TOTAL EQUITY CAPITAL',\n",
    "    'NET INCOME',\n",
    "    'NET INCOME (LOSS)'\n",
    "    ]\n",
    "\n",
    "def get_name_from_full_code(fullcode, df):\n",
    "    '''get column name from full code found in data dictionary'''\n",
    "    column = df['Item Name'][df['Full Code'] == fullcode]\n",
    "    col_name = column.iloc[0]\n",
    "    return col_name\n",
    "\n",
    "\n",
    "#Loop over short data dictionary and populate column mapper with name\n",
    "for code in short_dddf['Full Code']:\n",
    "    column_mapper[code] = get_name_from_full_code(code,short_dddf)\n",
    "\n",
    "\n",
    "def disambiguate_column_mapper(column_mapper, special_columns):\n",
    "    '''Loop over column mapper and replace ambigious columns (found in special_columns)'''\n",
    "\n",
    "    for key,value in column_mapper.items():\n",
    "        if value in special_columns:\n",
    "            column_mapper[key] = f'{value}: {key}' \n",
    "    return column_mapper\n",
    "\n",
    "#Final column mapper\n",
    "disambig_column_mapper = disambiguate_column_mapper(column_mapper, special_columns)\n",
    "\n",
    "#check to make sure columns have been renamed\n",
    "column_name_in_question = 'NET INCOME'\n",
    "for key,value in disambig_column_mapper.items():\n",
    "    if column_name_in_question in str(value):\n",
    "        print(value)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_df = df.rename(columns=disambig_column_mapper)\n",
    "renamed_dfq2 = dfq2.rename(columns=disambig_column_mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NET INCOME (LOSS)', 'NET INCOME (LOSS)',\n",
      "       'NET INCOME BEFORE INCOME TAXES; EXTRAORDINARY ITEMS; AND OTHER ADJUSTMENTS ON A FULLY TAXABLE EQUIVALENT BASIS',\n",
      "       'NET INCOME', 'NET INCOME',\n",
      "       'LESS: NET INCOME (LOSS) ATTRIBUTABLE TO NONCONTROLLING (MINORITY) INTERESTS',\n",
      "       'NET INCOME (LOSS) ATTRIBUTABLE TO BANK AND NONCONTROLLING MINORITY INTERESTS',\n",
      "       'CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - PROVISION FOR DEFERRED INCOME TAXES',\n",
      "       'CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - (GAIN) OR LOSS ON SALES OF ASSETS',\n",
      "       'CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - EQUITY IN UNDISTRIBUTED (EARNINGS) LOSSES OF SUBSIDIARIES',\n",
      "       'CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - NET CHANGE IN OTHER LIABILITIES',\n",
      "       'CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - NET CHANGE IN OTHER ASSETS',\n",
      "       'CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - OTHER; NET',\n",
      "       'CASH FLOWS FROM OPERATING ACTIVITIES: ADJUSTMENTS TO RECONCILE NET INCOME TO NET CASH PROVIDED BY OPERATING ACTIVITIES - TOTAL ADJUSTMENTS',\n",
      "       'NET INCOME (LOSS)', 'NET INCOME (LOSS)', 'NET INCOME (LOSS)',\n",
      "       'NET INCOME (LOSS)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Column Finder\n",
    "\n",
    "#print(renamed_dfq2.columns.str.contains('NAME'))\n",
    "\n",
    "search_string = 'NET INCOME'\n",
    "matching_columns = renamed_dfq2.columns[renamed_dfq2.columns.str.contains(search_string)]\n",
    "print(matching_columns)\n",
    "#print(renamed_dfq2['REPORTING DATE (CC;YR;MO;DA)'])\n",
    "# i = 0\n",
    "# for name in renamed_dfq2['ENTITY SHORT NAME']:\n",
    "#     i += 1\n",
    "#     if i > 50:\n",
    "#         break\n",
    "#     else:\n",
    "#         print(name)\n",
    "#target_bank = 'FIRST NAT OF NE'\n",
    "#print(renamed_dfq2[renamed_dfq2[\"ENTITY SHORT NAME\"] == target_bank])\n",
    "#print(renamed_df['AVERAGE TOTAL ASSETS (NET OF DEDUCTIONS)'].loc[2])\n",
    "#print(renamed_dfq2['AVERAGE TOTAL ASSETS (NET OF DEDUCTIONS)'].loc[5])\n",
    "#print(renamed_df[\"ENTITY SHORT NAME\"].loc[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so what needs to happen now? I have two dataframes both of which have the same columns and I want to mix them together in such a way that I can compare values across time\n",
    "\n",
    "I have a reporting date which matches the file date which is good. I could union the data and use the bank name and reporting date to compare over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to union\n",
    "union_df = pd.concat([renamed_df, renamed_dfq2], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        assets  dates  net_income\n",
      "ENTITY SHORT NAME                                \n",
      "ACNB CORP             391017.0      2      -141.5\n",
      "CAPITAL ONE FC      87248851.5      2   1517926.0\n",
      "CHOICEONE FS          429414.5      2       448.0\n",
      "CONNECTONE BC        1374685.0      2      4542.0\n",
      "EVERBANK FNCL CORP   3704783.0      2    -37161.0\n",
      "FIRST BUSEY CORP     2296076.0      2     16285.0\n",
      "HOPE BC              2192075.0      2     -9020.0\n",
      "NBT BC               1685470.5      2     -4546.0\n",
      "OLD NAT BC           7330520.5      2    -33795.0\n",
      "UNITED BSHRS         5339495.0      4     -9785.0\n",
      "WESBANCO             3800399.5      2    -33155.0\n"
     ]
    }
   ],
   "source": [
    "#print(union_df[union_df['ENTITY SHORT NAME'] == target_bank])\n",
    "filtered_df = union_df[['ENTITY SHORT NAME','TOTAL EQUITY CAPITAL: BHCK3210','NET INCOME (LOSS): BHBC4340','REPORTING DATE (CC;YR;MO;DA)']].copy()\n",
    "#filtered_df = filtered_df[filtered_df['TOTAL ASSETS'] > 0]\n",
    "#print(filtered_df.head())\n",
    "grouped_df = filtered_df.groupby(['ENTITY SHORT NAME']).agg(\n",
    "    assets = ('TOTAL EQUITY CAPITAL: BHCK3210',\"mean\"),\n",
    "    dates = ('REPORTING DATE (CC;YR;MO;DA)' , 'count'),\n",
    "    net_income = ('NET INCOME (LOSS): BHBC4340', 'mean')\n",
    ")\n",
    "net_income_df = grouped_df[(np.isnan(grouped_df['net_income']) == False) & (grouped_df['net_income'] != 0)]\n",
    "print(net_income_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     NET INCOME (LOSS): BHBC4340  REPORTING DATE (CC;YR;MO;DA)\n",
      "43                      -66310.0                      20250331\n",
      "767                          0.0                      20250630\n"
     ]
    }
   ],
   "source": [
    "#Calculate volitility of earnings\n",
    "\n",
    "VOE_bank = 'WESBANCO'\n",
    "VOE_df = union_df[union_df['ENTITY SHORT NAME'] == VOE_bank]\n",
    "\n",
    "print(VOE_df[['NET INCOME (LOSS): BHBC4340','REPORTING DATE (CC;YR;MO;DA)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     entity      std_abs  mean_abs        cv  std_pctchg\n",
      "0  WESBANCO  46888.25066  -33155.0 -1.414214         NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/1lp6v8rx7hg46v35nrbf4mk00000gn/T/ipykernel_7843/2939093299.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  volatility = vol_df.groupby('entity').apply(compute_volatility).reset_index()\n"
     ]
    }
   ],
   "source": [
    "#CHAT GPT Code  \n",
    "s = (VOE_df['REPORTING DATE (CC;YR;MO;DA)']\n",
    "     .astype(str)\n",
    "     .str.strip()\n",
    "     .str.replace(r'\\D+', '', regex=True)   # remove any separators just in case\n",
    "     .str.zfill(8))\n",
    "\n",
    "vol_df = VOE_df.rename(columns={\n",
    "    'ENTITY SHORT NAME': 'entity',\n",
    "    'NET INCOME (LOSS): BHBC4340': 'net_income',\n",
    "    'REPORTING DATE (CC;YR;MO;DA)': 'report_date'\n",
    "})\n",
    "\n",
    "# Convert types\n",
    "vol_df['report_date'] = pd.to_datetime(s, format=\"%Y%m%d\", errors='raise')  # adjust format if needed\n",
    "vol_df = vol_df.sort_values(['entity','report_date'])\n",
    "\n",
    "# Group by entity and compute volatility metrics\n",
    "def compute_volatility(g):\n",
    "    # Ensure sorted by date\n",
    "    g = g.set_index('report_date').sort_index()\n",
    "    eps = g['net_income'].dropna()\n",
    "    pctchg = eps.pct_change().dropna()\n",
    "    return pd.Series({\n",
    "        'std_abs': eps.std(),\n",
    "        'mean_abs': eps.mean(),\n",
    "        'cv': (eps.std() / eps.mean()) if eps.mean()!=0 else None,\n",
    "        'std_pctchg': pctchg.std()\n",
    "    })\n",
    "\n",
    "volatility = vol_df.groupby('entity').apply(compute_volatility).reset_index()\n",
    "\n",
    "print(volatility.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
