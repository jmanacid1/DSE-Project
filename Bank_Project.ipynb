{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: ascii\n"
     ]
    }
   ],
   "source": [
    "import chardet as chardet\n",
    "with open('Data/BHCF20250331.txt', 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        encoding = result['encoding']\n",
    "        print(f\"Detected encoding: {encoding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/1lp6v8rx7hg46v35nrbf4mk00000gn/T/ipykernel_3191/2334954582.py:1: DtypeWarning: Columns (2088,2089,2091,2095,2097,2098,2099,2100,2101,2132,2141,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2165,2166,2167,2171,2174,2178,2184,2185,2186,2187,2200,2201,2202,2203,2204,2205,2206,2208,2216,2219,2223) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dftest = pd.read_csv('Data/BHCF20241231.txt', delimiter = '^',encoding='ISO-8859-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RSSD9001  RSSD9999  RSSD9007  RSSD9008  RSSD9132  RSSD9032  RSSD9146  \\\n",
      "0   1020180  20241231  20230522  20250430    551111         9       1.0   \n",
      "1   1020201  20241231  20240306  99991231    551111         7       2.0   \n",
      "2   1020395  20241231  20170517  99991231    551111         6       1.0   \n",
      "3   1020582  20241231  20181231  99991231    551111         7       1.0   \n",
      "4   1020667  20241231  20120930  20241231    551111         7       1.0   \n",
      "\n",
      "   BHBC3368  BHBC3402  BHBC3516  ...  TEXTB056  TEXTC497  TEXTFT29  TEXTFT31  \\\n",
      "0       NaN       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "1       NaN       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "2       NaN       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "3       NaN       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "4       NaN       NaN       NaN  ...       NaN       NaN       NaN       NaN   \n",
      "\n",
      "            RSSD4087  RSSD9220             TEXTC703     TEXTC708  TEXTC714  \\\n",
      "0     WWW.BREMER.COM     55102  Ernst and Young LLP  Minneapolis        MN   \n",
      "1                  0     10001                  NaN          NaN       NaN   \n",
      "2                NaN     36420  Mauldin and Jenkins   Birmingham        AL   \n",
      "3  WWW.WOODTRUST.COM     54494                  NaN          NaN       NaN   \n",
      "4                  0     52806                  NaN          NaN       NaN   \n",
      "\n",
      "   TEXTC715  \n",
      "0   55402.0  \n",
      "1       NaN  \n",
      "2   35209.0  \n",
      "3       NaN  \n",
      "4       NaN  \n",
      "\n",
      "[5 rows x 2224 columns]\n"
     ]
    }
   ],
   "source": [
    "dftest = pd.read_csv('Data/BHCF20241231.txt', delimiter = '^',encoding='ISO-8859-1')\n",
    "print(dftest.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/1lp6v8rx7hg46v35nrbf4mk00000gn/T/ipykernel_3191/1870212365.py:6: DtypeWarning: Columns (2088,2089,2091,2092,2094,2095,2098,2099,2100,2101,2132,2141,2151,2152,2153,2154,2155,2156,2157,2158,2160,2161,2162,2163,2165,2166,2167,2171,2173,2174,2177,2178,2179,2181,2183,2184,2185,2200,2201,2202,2203,2204,2205,2206,2207,2208,2215,2216,2217,2219,2220,2221,2222) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, delimiter = '^', encoding='ISO-8859-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/BHCF20250630 3.txt (3749, 2224)\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at row 3589",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m df_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m all_files:\n\u001b[0;32m----> 6\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m^\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mISO-8859-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     df_list\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(file_path, df\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 3589"
     ]
    }
   ],
   "source": [
    "import glob as glob\n",
    "all_files = glob.glob('Data/BHCF*.txt')\n",
    "\n",
    "df_list = []\n",
    "for file_path in all_files:\n",
    "    df = pd.read_csv(file_path, delimiter = '^', encoding='ISO-8859-1')\n",
    "    df_list.append(df)\n",
    "    print(file_path, df.shape)\n",
    "combined_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8070, 2224)\n"
     ]
    }
   ],
   "source": [
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/1lp6v8rx7hg46v35nrbf4mk00000gn/T/ipykernel_7843/2340049902.py:1: DtypeWarning: Columns (2088,2089,2092,2097,2100,2101,2141,2152,2153,2154,2157,2158,2167,2185,2201,2202,2203,2204,2205,2206,2207,2208,2216,2219,2220,2221,2222) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('Data/BHCF20250331.txt', delimiter = '^')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Data/BHCF20250331.txt', delimiter = '^')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/1lp6v8rx7hg46v35nrbf4mk00000gn/T/ipykernel_7843/3343375304.py:1: DtypeWarning: Columns (2088,2089,2091,2092,2094,2095,2098,2099,2100,2101,2132,2141,2151,2152,2153,2154,2155,2156,2157,2158,2160,2161,2162,2163,2165,2166,2167,2171,2173,2174,2177,2178,2179,2181,2183,2184,2185,2200,2201,2202,2203,2204,2205,2206,2207,2208,2215,2216,2217,2219,2220,2221,2222) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfq2 = pd.read_csv('Data/BHCF20250630 3.txt', delimiter = '^')\n"
     ]
    }
   ],
   "source": [
    "dfq2 = pd.read_csv('Data/BHCF20250630 3.txt', delimiter = '^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['RSSD', 'BHBC', 'BHCA', 'BHCB', 'BHCK', 'BHCM', 'BHCP', 'BHCT', 'BHCW',\n",
      "       'BHCX', 'BHCY', 'BHDM', 'BHFN', 'BHOD', 'BHPA', 'BHPX', 'BHSP', 'BHSX',\n",
      "       'BHTX', 'TEXT'],\n",
      "      dtype='object')\n",
      "Index(['RSSD', 'BHBC', 'BHCA', 'BHCB', 'BHCK', 'BHCM', 'BHCP', 'BHCT', 'BHCW',\n",
      "       'BHCX', 'BHCY', 'BHDM', 'BHFN', 'BHOD', 'BHPA', 'BHPX', 'BHSP', 'BHSX',\n",
      "       'BHTX', 'TEXT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.str[:4].unique())\n",
    "print(combined_df.columns.str[:4].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dddf = pd.read_csv('/Users/jacksonlipfert/Downloads/MDRM/MDRM_CSV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68622    ORGANIZATION TYPE\n",
      "Name: Item Name, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/1lp6v8rx7hg46v35nrbf4mk00000gn/T/ipykernel_3191/474950684.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  short_dddf['Full Code'] = short_dddf['Mnemonic'] + short_dddf['Item Code']\n"
     ]
    }
   ],
   "source": [
    "#dddf.head()\n",
    "short_dddf = dddf[['Mnemonic','Item Code','Item Name']]\n",
    "\n",
    "\n",
    "short_dddf['Full Code'] = short_dddf['Mnemonic'] + short_dddf['Item Code']\n",
    "important_codes = ['RSSD', 'BHBC', 'BHCA', 'BHCB', 'BHCK', 'BHCM', 'BHCP', 'BHCT', 'BHCW',\n",
    "       'BHCX', 'BHCY', 'BHDM', 'BHFN', 'BHOD', 'BHPA', 'BHPX', 'BHSP', 'BHSX',\n",
    "       'BHTX', 'TEXT']\n",
    "short_dddf = short_dddf[short_dddf['Mnemonic'].isin(important_codes)]\n",
    "\n",
    "print(short_dddf['Item Name'][short_dddf['Item Code'] == '9047'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIER 1 RISK-BASED CAPITAL RATIO: BHCA7206\n",
      "TIER 1 RISK-BASED CAPITAL RATIO: BHCK7206\n",
      "TIER 1 RISK-BASED CAPITAL RATIO: BHCW7206\n"
     ]
    }
   ],
   "source": [
    "# create mapper\n",
    "\n",
    "\n",
    "column_mapper = {}\n",
    "\n",
    "#Columns that appear across multiple types of companies with the same balance sheet name, need to keep code to disambiguate\n",
    "special_columns = [\n",
    "    'TOTAL ASSETS',\n",
    "    'TOTAL EQUITY',\n",
    "    'TOTAL EQUITY CAPITAL',\n",
    "    'NET INCOME',\n",
    "    'NET INCOME (LOSS)',\n",
    "    'TIER 1 RISK-BASED CAPITAL RATIO',\n",
    "    'RISK-WEIGHTED ASSETS (NET OF ALLOWANCES AND OTHER DEDUCTIONS)'\n",
    "    ]\n",
    "\n",
    "def get_name_from_full_code(fullcode, df):\n",
    "    '''get column name from full code found in data dictionary'''\n",
    "    column = df['Item Name'][df['Full Code'] == fullcode]\n",
    "    col_name = column.iloc[0]\n",
    "    return col_name\n",
    "\n",
    "\n",
    "#Loop over short data dictionary and populate column mapper with name\n",
    "for code in short_dddf['Full Code']:\n",
    "    column_mapper[code] = get_name_from_full_code(code,short_dddf)\n",
    "\n",
    "\n",
    "def disambiguate_column_mapper(column_mapper, special_columns):\n",
    "    '''Loop over column mapper and replace ambigious columns (found in special_columns)'''\n",
    "\n",
    "    for key,value in column_mapper.items():\n",
    "        if value in special_columns:\n",
    "            column_mapper[key] = f'{value}: {key}' \n",
    "    return column_mapper\n",
    "\n",
    "def disambiguate_all_column_mapper(column_mapper):\n",
    "    '''Loop over column mapper and replace ambigious columns (found in special_columns)'''\n",
    "\n",
    "    for key,value in column_mapper.items():\n",
    "        column_mapper[key] = f'{value}: {key}' \n",
    "    return column_mapper\n",
    "\n",
    "#Final column mapper\n",
    "#disambig_column_mapper = disambiguate_column_mapper(column_mapper, special_columns)\n",
    "disambig_column_mapper = disambiguate_all_column_mapper(column_mapper)\n",
    "\n",
    "#check to make sure columns have been renamed\n",
    "column_name_in_question = 'TIER 1 RISK-BASED CAPITAL RATIO'\n",
    "for key,value in disambig_column_mapper.items():\n",
    "    if column_name_in_question in str(value):\n",
    "        print(value)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_df = df.rename(columns=disambig_column_mapper)\n",
    "#renamed_dfq2 = dfq2.rename(columns=disambig_column_mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m renamed_combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_df\u001b[49m\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39mdisambig_column_mapper)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(renamed_combined_df\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combined_df' is not defined"
     ]
    }
   ],
   "source": [
    "renamed_combined_df = combined_df.rename(columns=disambig_column_mapper)\n",
    "print(renamed_combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ORGANIZATION TYPE: RSSD9047'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Column Finder\n",
    "\n",
    "#print(renamed_dfq2.columns.str.contains('NAME'))\n",
    "\n",
    "search_string = 'RSSD9047'\n",
    "matching_columns = renamed_df.columns[renamed_df.columns.str.contains(search_string)]\n",
    "print(matching_columns)\n",
    "#print(renamed_dfq2['REPORTING DATE (CC;YR;MO;DA)'])\n",
    "# i = 0\n",
    "# for name in renamed_dfq2['ENTITY SHORT NAME']:\n",
    "#     i += 1\n",
    "#     if i > 50:\n",
    "#         break\n",
    "#     else:\n",
    "#         print(name)\n",
    "#target_bank = 'FIRST NAT OF NE'\n",
    "#print(renamed_dfq2[renamed_dfq2[\"ENTITY SHORT NAME\"] == target_bank])\n",
    "#print(renamed_df['AVERAGE TOTAL ASSETS (NET OF DEDUCTIONS)'].loc[2])\n",
    "#print(renamed_dfq2['AVERAGE TOTAL ASSETS (NET OF DEDUCTIONS)'].loc[5])\n",
    "#print(renamed_df[\"ENTITY SHORT NAME\"].loc[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ORGANIZATION TYPE: RSSD9047\n",
       "1     3510\n",
       "6      140\n",
       "11      62\n",
       "3       27\n",
       "4        3\n",
       "13       3\n",
       "2        2\n",
       "99       1\n",
       "12       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter just to BHCs\n",
    "\n",
    "#Think we want 1 only\n",
    "renamed_df[\"ORGANIZATION TYPE: RSSD9047\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so what needs to happen now? I have two dataframes both of which have the same columns and I want to mix them together in such a way that I can compare values across time\n",
    "\n",
    "I have a reporting date which matches the file date which is good. I could union the data and use the bank name and reporting date to compare over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine Bank Health\n",
    "renamed_df[\"Tier 1 Capital Ratio\"] = renamed_df['TIER 1 RISK-BASED CAPITAL RATIO: BHCW7206']/renamed_df['RISK-WEIGHTED ASSETS (NET OF ALLOWANCES AND OTHER DEDUCTIONS): BHCAA223']\n",
    "\n",
    "renamed_df['TIER 1 RISK-BASED CAPITAL RATIO: BHCW7206'][renamed_df['TIER 1 RISK-BASED CAPITAL RATIO: BHCW7206'].isna() == False].shape\n",
    "\n",
    "renamed_df['RISK-WEIGHTED ASSETS (NET OF ALLOWANCES AND OTHER DEDUCTIONS): BHCAA223'][renamed_df['RISK-WEIGHTED ASSETS (NET OF ALLOWANCES AND OTHER DEDUCTIONS): BHCAA223'].isna() == False].shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to union\n",
    "union_df = pd.concat([renamed_df, renamed_dfq2], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        assets  dates  net_income\n",
      "ENTITY SHORT NAME                                \n",
      "ACNB CORP             391017.0      2      -141.5\n",
      "CAPITAL ONE FC      87248851.5      2   1517926.0\n",
      "CHOICEONE FS          429414.5      2       448.0\n",
      "CONNECTONE BC        1374685.0      2      4542.0\n",
      "EVERBANK FNCL CORP   3704783.0      2    -37161.0\n",
      "FIRST BUSEY CORP     2296076.0      2     16285.0\n",
      "HOPE BC              2192075.0      2     -9020.0\n",
      "NBT BC               1685470.5      2     -4546.0\n",
      "OLD NAT BC           7330520.5      2    -33795.0\n",
      "UNITED BSHRS         5339495.0      4     -9785.0\n",
      "WESBANCO             3800399.5      2    -33155.0\n"
     ]
    }
   ],
   "source": [
    "#print(union_df[union_df['ENTITY SHORT NAME'] == target_bank])\n",
    "filtered_df = union_df[['ENTITY SHORT NAME','TOTAL EQUITY CAPITAL: BHCK3210','NET INCOME (LOSS): BHBC4340','REPORTING DATE (CC;YR;MO;DA)']].copy()\n",
    "#filtered_df = filtered_df[filtered_df['TOTAL ASSETS'] > 0]\n",
    "#print(filtered_df.head())\n",
    "grouped_df = filtered_df.groupby(['ENTITY SHORT NAME']).agg(\n",
    "    assets = ('TOTAL EQUITY CAPITAL: BHCK3210',\"mean\"),\n",
    "    dates = ('REPORTING DATE (CC;YR;MO;DA)' , 'count'),\n",
    "    net_income = ('NET INCOME (LOSS): BHBC4340', 'mean')\n",
    ")\n",
    "net_income_df = grouped_df[(np.isnan(grouped_df['net_income']) == False) & (grouped_df['net_income'] != 0)]\n",
    "print(net_income_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     NET INCOME (LOSS): BHBC4340  REPORTING DATE (CC;YR;MO;DA)\n",
      "43                      -66310.0                      20250331\n",
      "767                          0.0                      20250630\n"
     ]
    }
   ],
   "source": [
    "#Calculate volitility of earnings\n",
    "\n",
    "VOE_bank = 'WESBANCO'\n",
    "VOE_df = union_df[union_df['ENTITY SHORT NAME'] == VOE_bank]\n",
    "\n",
    "print(VOE_df[['NET INCOME (LOSS): BHBC4340','REPORTING DATE (CC;YR;MO;DA)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     entity      std_abs  mean_abs        cv  std_pctchg\n",
      "0  WESBANCO  46888.25066  -33155.0 -1.414214         NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/1lp6v8rx7hg46v35nrbf4mk00000gn/T/ipykernel_7843/2939093299.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  volatility = vol_df.groupby('entity').apply(compute_volatility).reset_index()\n"
     ]
    }
   ],
   "source": [
    "#CHAT GPT Code  \n",
    "s = (VOE_df['REPORTING DATE (CC;YR;MO;DA)']\n",
    "     .astype(str)\n",
    "     .str.strip()\n",
    "     .str.replace(r'\\D+', '', regex=True)   # remove any separators just in case\n",
    "     .str.zfill(8))\n",
    "\n",
    "vol_df = VOE_df.rename(columns={\n",
    "    'ENTITY SHORT NAME': 'entity',\n",
    "    'NET INCOME (LOSS): BHBC4340': 'net_income',\n",
    "    'REPORTING DATE (CC;YR;MO;DA)': 'report_date'\n",
    "})\n",
    "\n",
    "# Convert types\n",
    "vol_df['report_date'] = pd.to_datetime(s, format=\"%Y%m%d\", errors='raise')  # adjust format if needed\n",
    "vol_df = vol_df.sort_values(['entity','report_date'])\n",
    "\n",
    "# Group by entity and compute volatility metrics\n",
    "def compute_volatility(g):\n",
    "    # Ensure sorted by date\n",
    "    g = g.set_index('report_date').sort_index()\n",
    "    eps = g['net_income'].dropna()\n",
    "    pctchg = eps.pct_change().dropna()\n",
    "    return pd.Series({\n",
    "        'std_abs': eps.std(),\n",
    "        'mean_abs': eps.mean(),\n",
    "        'cv': (eps.std() / eps.mean()) if eps.mean()!=0 else None,\n",
    "        'std_pctchg': pctchg.std()\n",
    "    })\n",
    "\n",
    "volatility = vol_df.groupby('entity').apply(compute_volatility).reset_index()\n",
    "\n",
    "print(volatility.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
